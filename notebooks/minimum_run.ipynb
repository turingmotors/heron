{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266046e1-9bed-4234-abaf-8f03707dc639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5266932ad34c6e81ca44b2a7ce6ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Inoichan/GIT-Llama-2-7B were not used when initializing GitLlamaForCausalLM: ['model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing GitLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GitLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/opt/conda/envs/heron/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/heron/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> ##Instruction: Please answer the following question concletely. ##Question: What is unusual about this image? Explain precisely and concletely what he is doing? ##Answer:  The person in the image is using a device to lift a car, which is unusual. He is also standing in the middle of the road, which is also unusual.</s>']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "from heron.models.git_llm.git_llama import GitLlamaForCausalLM\n",
    "\n",
    "device_id = 0\n",
    "\n",
    "# prepare a pretrained model\n",
    "model = GitLlamaForCausalLM.from_pretrained('Inoichan/GIT-Llama-2-7B')\n",
    "model.eval()\n",
    "model.to(f\"cuda:{device_id}\")\n",
    "\n",
    "# prepare a processor\n",
    "processor = AutoProcessor.from_pretrained('Inoichan/GIT-Llama-2-7B')\n",
    "\n",
    "# prepare inputs\n",
    "url = \"https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "text = f\"##Instruction: Please answer the following question concletely. ##Question: What is unusual about this image? Explain precisely and concletely what he is doing? ##Answer: \"\n",
    "\n",
    "# do preprocessing\n",
    "inputs = processor(\n",
    "    text,\n",
    "    image,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    ")\n",
    "inputs = {k: v.to(f\"cuda:{device_id}\") for k, v in inputs.items()}\n",
    "\n",
    "# set eos token\n",
    "eos_token_id_list = [\n",
    "    processor.tokenizer.pad_token_id,\n",
    "    processor.tokenizer.eos_token_id,\n",
    "]\n",
    "\n",
    "# do inference\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_length=256, do_sample=False, temperature=0., eos_token_id=eos_token_id_list)\n",
    "\n",
    "# print result\n",
    "print(processor.tokenizer.batch_decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfbb25-adf9-44b9-9771-055a01a48f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06974580-dc74-4811-91cb-3cd6cce4275b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
