{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef69431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2410943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-08 06:38:34,481] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from heron.models.video_blip import VideoBlipForConditionalGeneration, VideoBlipProcessor\n",
    "from transformers import LlamaTokenizer\n",
    "import wandb\n",
    "\n",
    "device_id = 0\n",
    "device = f\"cuda:{device_id}\"\n",
    "\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13242f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b69d63148f4820bf917f955aedc853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe2676936bc4ac090a8fe6554b28f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoBlipForConditionalGeneration were not initialized from the model checkpoint at /mnt/disks/disk2/model_out/stablelm-beta/abci-exp001 and are newly initialized because the shapes did not match:\n",
      "- text_projection.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- text_projection.weight: found shape torch.Size([2560, 768]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1\"\n",
    "model = VideoBlipForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16, ignore_mismatched_sizes=True\n",
    ")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"novelai/nerdstash-tokenizer-v1\", additional_special_tokens=['▁▁'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.half()\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8edaeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a processor\n",
    "processor = VideoBlipProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "processor.tokenizer = tokenizer\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "def generate_response(question, image):\n",
    "    # prepare inputs\n",
    "    text = f\"##human: {question}\\n##gpt: \"\n",
    "\n",
    "    # do preprocessing\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(device, torch.float16)\n",
    "    \n",
    "    # set eos token\n",
    "    eos_token_id_list = [\n",
    "        processor.tokenizer.pad_token_id,\n",
    "        processor.tokenizer.eos_token_id,\n",
    "        int(tokenizer.convert_tokens_to_ids(\"\\n\"))\n",
    "    ]\n",
    "\n",
    "    # do inference\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_length=256, do_sample=False, temperature=0., eos_token_id=eos_token_id_list, no_repeat_ngram_size=2)\n",
    "    res = processor.tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e6c4b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def load_q(p):\n",
    "    data = []\n",
    "    for line in open(p):\n",
    "        data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "q_data = load_q(\"qa90_questions_ja.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "993fcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_data(q_data):\n",
    "    result = []\n",
    "    for q in q_data:\n",
    "        image = Image.open(\"val2014/COCO_val2014_\" + q[\"image\"])\n",
    "        question = q[\"text_JA\"]\n",
    "        display(image)\n",
    "        res = generate_response(question, image)\n",
    "        print(question)\n",
    "        if \"##\" in res:\n",
    "            res = res.split(\"##\")[0]\n",
    "        print(\"final\", res)\n",
    "        q[\"answer\"] = res\n",
    "        result.append(q)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = answer_data(q_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5596a621-706b-4252-8441-2737c784e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"stablelm-alpha-exp001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandbに結果をアップロードしたい場合\n",
    "def upload_result(result, name):\n",
    "    wandb.init(project=\"heron-eval\", name=name)\n",
    "    table = wandb.Table(columns=['ID', 'Image', 'Question', 'Answer'])\n",
    "    for r in result:\n",
    "        image = Image.open(\"val2014/COCO_val2014_\" + r[\"image\"])\n",
    "        answer = r[\"answer\"]\n",
    "        img = wandb.Image(image, caption=answer)\n",
    "        idx = r[\"question_id\"]\n",
    "        table.add_data(idx, img, r[\"text_JA\"], answer)\n",
    "    wandb.log({\"Table\" : table})\n",
    "    \n",
    "upload_result(result, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7246349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(jsonl, model_name):\n",
    "    with open(f\"{model_name}_answer.jsonl\", \"w\") as f:\n",
    "        for r in jsonl:\n",
    "            f.write(json.dumps(r)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1b930312",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(result, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
